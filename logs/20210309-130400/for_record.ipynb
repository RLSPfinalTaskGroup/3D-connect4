{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ScoreFour.ipynb","provenance":[{"file_id":"1JFgYobx-m0cqTRZwRv-uEN5m-zJG-iQ9","timestamp":1614992719457},{"file_id":"1jZyoHU8SFD0rF04k8nsu-lKJLiiAv_t7","timestamp":1614962802098}],"collapsed_sections":["lKXC26EvI8jL","ZHtB9b8gk4G_","0h_RESMpSU4e"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"favOHMTeI8ZY"},"source":["# 強化学習を用いた3D Score Fourの攻略"]},{"cell_type":"markdown","metadata":{"id":"lKXC26EvI8jL"},"source":["## 各種設定"]},{"cell_type":"code","metadata":{"id":"IxVJPsJEVPP7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615262532768,"user_tz":-540,"elapsed":665,"user":{"displayName":"kath mandu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijJDLCkA__3fFgGa09ZQUXu3DDTcT8QTwF2rBq=s64","userId":"12009895993958536903"}},"outputId":"21857ce6-1bb4-4de2-ec13-4b43dee879ab"},"source":["# Driveのマウント(logをドライブに保存)\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"x4w_pfd1-T2m","executionInfo":{"status":"ok","timestamp":1615262532978,"user_tz":-540,"elapsed":871,"user":{"displayName":"kath mandu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijJDLCkA__3fFgGa09ZQUXu3DDTcT8QTwF2rBq=s64","userId":"12009895993958536903"}}},"source":["# 各々のフォルダ\r\n","each_dir = \"/content/drive/MyDrive/Colab Notebooks/MatsuoSeminer/Research\""],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"4axWZO3Tk8zj","executionInfo":{"status":"ok","timestamp":1615262533598,"user_tz":-540,"elapsed":1488,"user":{"displayName":"kath mandu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijJDLCkA__3fFgGa09ZQUXu3DDTcT8QTwF2rBq=s64","userId":"12009895993958536903"}}},"source":["# ライブラリのインポート\r\n","\r\n","import gym\r\n","from gym import error, spaces, utils\r\n","from gym.utils import seeding\r\n","\r\n","import plotly.express as px\r\n","import pandas as pd\r\n","import numpy as np\r\n","from google.colab import output\r\n","import random\r\n","import time\r\n","\r\n","from typing import List, Tuple, Union, Iterable\r\n","\r\n","import torch\r\n","from torch import nn, optim\r\n","from torch.utils.tensorboard import SummaryWriter\r\n","\r\n","from datetime import datetime, timedelta, timezone\r\n","import os\r\n","from stat import SF_IMMUTABLE\r\n","\r\n","import shutil"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"RcY4SgSSs-Dl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615262534055,"user_tz":-540,"elapsed":1943,"user":{"displayName":"kath mandu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijJDLCkA__3fFgGa09ZQUXu3DDTcT8QTwF2rBq=s64","userId":"12009895993958536903"}},"outputId":"6cc42671-26a0-44fe-acc6-add72dde8503"},"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["cuda\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PsSy-ze4tBt7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615262534056,"user_tz":-540,"elapsed":1942,"user":{"displayName":"kath mandu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijJDLCkA__3fFgGa09ZQUXu3DDTcT8QTwF2rBq=s64","userId":"12009895993958536903"}},"outputId":"95374c2b-4111-42d2-84e1-b0ca1def7ca6"},"source":["!nvidia-smi"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Tue Mar  9 04:02:12 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.56       Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   65C    P8    11W /  70W |      3MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kzlClBZzcyXe","executionInfo":{"status":"ok","timestamp":1615262534056,"user_tz":-540,"elapsed":1940,"user":{"displayName":"kath mandu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijJDLCkA__3fFgGa09ZQUXu3DDTcT8QTwF2rBq=s64","userId":"12009895993958536903"}}},"source":["# Seed値の固定\r\n","\r\n","def fix_seed(seed):\r\n","    os.environ[\"PYTHONHASHSEED\"] = str(seed)\r\n","    # random\r\n","    random.seed(seed)\r\n","    # Numpy\r\n","    np.random.seed(seed)\r\n","    # Pytorch\r\n","    torch.manual_seed(seed)\r\n","    torch.cuda.manual_seed_all(seed)\r\n","    torch.backends.cudnn.deterministic = True\r\n","    torch.backends.cudnn.benchmark = False # 処理速度は落ちる"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZHtB9b8gk4G_"},"source":["## 立体四目並べ OpenAIGym形式"]},{"cell_type":"markdown","metadata":{"id":"eDZ7BRqIbW0V"},"source":["cubeとboardの違い\r\n","- cube.shape = (self.num_grid,self.num_grid,self.num_grid)  \r\n","- board.shape = (1,self.num_grid,self.num_grid,self.num_grid)\r\n","\r\n","初期化時の引数\r\n","\r\n","|引数|内容|  \r\n","|:--------:|:--------:|\r\n","|num_grid|一辺の長さ|\r\n","|num_win_seq|勝利条件(この数一列に並んだら勝利)|\r\n","|win_reward|勝利時の報酬|\r\n","|draw_penalty|引き分け時のペナルティ|\r\n","|lose_penalty|敗北時のペナルティ|\r\n","|couldnt_penalty|おけない場所を選択した時のペナルティ|\r\n","|time_penalty|使用していない|\r\n","|first_player|先攻|\r\n","\r\n","- 報酬・ペナルティについては全て正の値で設定すること\r\n","- ペナルティは-1を掛けた値を足します"]},{"cell_type":"code","metadata":{"id":"0rf7pMG3lHQj","executionInfo":{"status":"ok","timestamp":1615262534581,"user_tz":-540,"elapsed":2464,"user":{"displayName":"kath mandu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijJDLCkA__3fFgGa09ZQUXu3DDTcT8QTwF2rBq=s64","userId":"12009895993958536903"}}},"source":["class ScoreFour3dEnv(gym.Env):\r\n","  def __init__(self, num_grid=4, num_win_seq=4, win_reward=10, draw_penalty=5,lose_penalty=10, couldnt_penalty=1, time_penalty=0.1, first_player=1):\r\n","    super().__init__()\r\n","\r\n","    self.num_grid = num_grid\r\n","    self.num_win_seq = num_win_seq\r\n","    self.win_reward = win_reward\r\n","    self.draw_penalty = draw_penalty\r\n","    self.lose_penalty = lose_penalty # 今のところ使用しない\r\n","    self.couldnt_penalty = couldnt_penalty\r\n","    self.time_penalty = time_penalty\r\n","    \r\n","    # アクションの数の設定\r\n","    self.action_space = gym.spaces.Discrete(self.num_grid*self.num_grid)\r\n","    # 観測空間(state)を定義\r\n","    self.observation_space = gym.spaces.Box(low=-1, high=1, shape=(1, self.num_grid, self.num_grid, self.num_grid))\r\n","\r\n","    # 最初のプレーヤーがどちらかを定義\r\n","    self.player = first_player\r\n","\r\n","    # 判定用定数\r\n","    self.WIN_A = np.full(4,1)\r\n","    self.WIN_B = np.full(4,-1)\r\n","\r\n","    self.reset()\r\n","\r\n","  def reset(self):\r\n","    self.board = [[[[0]*self.num_grid for _ in range(self.num_grid)] for _ in range(self.num_grid)]]\r\n","    return torch.tensor(self.board).float()\r\n","\r\n","  def step(self, action):\r\n","    action = self.base_change(action, self.num_grid).zfill(2)\r\n","    W = int(action[0])\r\n","    D = int(action[1])\r\n","    reward = 0\r\n","    winner = 0\r\n","    done = False\r\n","    is_couldnt_locate=False\r\n","\r\n","\r\n","    for H in range(self.num_grid):\r\n","      if (self.board[0][H][W][D]==0): # 空いていたら置く\r\n","        self.board[0][H][W][D] = self.player\r\n","        self.player *= -1\r\n","        break\r\n","      elif (H == self.num_grid-1):\r\n","        # print(\"Couldn't Locate!!\")\r\n","        reward = -self.couldnt_penalty\r\n","        is_couldnt_locate=True\r\n","      else:\r\n","        pass\r\n","    \r\n","    done = self.is_game_end(np.array(self.board[0]))\r\n","\r\n","    if (done): # stepを施行した側は勝つ以外ありえない\r\n","      reward = self.win_reward\r\n","      winner = self.player*-1\r\n","    elif (not(0 in np.array(self.board[0]).flatten())): # draw\r\n","      done = True\r\n","      reward = -self.draw_penalty\r\n","    else:\r\n","      pass\r\n","\r\n","    info={\"turn\": self.player, \"winner\": winner, \"is_couldnt_locate\":is_couldnt_locate}\r\n","\r\n","    return torch.tensor(self.board).float(), reward, done, info\r\n","  \r\n","  # 入力をbaseで指定した進数に変換\r\n","  def base_change(self, value, base):\r\n","    if (int(value / base)):\r\n","      return self.base_change(int(value / base), base) + str(value % base)\r\n","    return str(value % base)\r\n","\r\n","\r\n","  def is_game_end(self, cube: np.ndarray) -> bool:\r\n","    num_stride = self.num_grid - self.num_win_seq + 1\r\n","\r\n","    # 1辺self.num_grudマスの格子内で、1辺self.num_win_seqマスのcubeを1マスずつずらしていく\r\n","    for dim_H_stride_id in range(num_stride):\r\n","      for dim_W_stride_id in range(num_stride):\r\n","        for dim_D_stride_id in range(num_stride):\r\n","          search_cube = cube[dim_H_stride_id:dim_H_stride_id+self.num_win_seq,\r\n","                        dim_W_stride_id:dim_W_stride_id+self.num_win_seq,\r\n","                        dim_D_stride_id:dim_D_stride_id+self.num_win_seq]\r\n","          \r\n","          # x,y,z軸各方向に垂直な面について解析\r\n","          cube_list = [search_cube, np.rot90(search_cube,axes=(0, 2)), np.rot90(search_cube,axes=(1, 2))] \r\n","\r\n","          # cube内の考えうる全ての二次元平面上でループ\r\n","          for each_cube in cube_list:\r\n","            for i in range(self.num_win_seq):\r\n","              # 2次元平面上でビンゴしていないか確認\r\n","              if self.is_end_on_2d_plane(each_cube[i]):\r\n","                return True\r\n","              if self.is_end_on_2d_plane(each_cube[i].T):\r\n","                return True\r\n","\r\n","          # 立体的な斜め\r\n","          for i in range(4):\r\n","            cube = np.rot90(cube)\r\n","            if (self.is_diag_on_3d_cube(cube)):\r\n","              return True\r\n","    \r\n","    return False\r\n","\r\n","\r\n","  # N×Nの2次元配列上でN個玉が並んでいるところがあるかを判定する関数。（ビンゴの判定みたいなもの）\r\n","  def is_end_on_2d_plane(self, plane: np.ndarray) -> bool:\r\n","    assert plane.shape == (self.num_win_seq, self.num_win_seq)\r\n","\r\n","    # 行\r\n","    for row in plane:\r\n","      if(all(row == self.WIN_A)):\r\n","        return True\r\n","      elif(all(row == self.WIN_B)):\r\n","        return True\r\n","    \r\n","    # 斜め(片側)\r\n","    oblique_elements = np.empty(0)\r\n","    for a in range(4):\r\n","      for b in range(4):\r\n","        if(a==b):\r\n","          oblique_elements = np.append(oblique_elements,plane[a][b])\r\n","\r\n","    if(all(oblique_elements == self.WIN_A)):\r\n","      return True\r\n","    elif(all(oblique_elements == self.WIN_B)):\r\n","      return True\r\n","\r\n","    return False\r\n","\r\n","\r\n","  # N×N×Nの3次元配列上で、N個の玉が立体対角上に並んでいるかどうかを判定する関数。\r\n","  def is_diag_on_3d_cube(self, cube: np.ndarray) -> bool:\r\n","    assert cube.shape == (self.num_win_seq, self.num_win_seq, self.num_win_seq)\r\n","\r\n","    oblique_elements = np.empty(0)\r\n","    for f in range(self.num_win_seq):\r\n","      for a in range(self.num_win_seq):\r\n","        for b in range(self.num_win_seq):\r\n","          if(f==a and a==b and f==b):\r\n","            oblique_elements = np.append(oblique_elements,cube[f][a][b])\r\n","\r\n","    if(all(oblique_elements == np.full(self.num_win_seq,1))):\r\n","      return True\r\n","    elif(all(oblique_elements == np.full(self.num_win_seq,-1))):\r\n","      return True\r\n","    return False\r\n","\r\n","\r\n","\r\n","  def render(self, mode = \"print\", isClear = False):\r\n","    if (isClear):\r\n","      output.clear() #出力の消去\r\n","    \r\n","    if (mode == \"print\"):\r\n","      i = 0\r\n","      for square in self.board[0]:\r\n","        print(\"{}F\".format(i))\r\n","        for line in square:\r\n","          print(line)\r\n","        i += 1\r\n","    \r\n","    elif (mode == \"plot\"):\r\n","      data = pd.DataFrame(index=[],columns=[\"W\",\"D\",\"H\",\"Player\"])\r\n","      index = 0\r\n","      for i in range(4):\r\n","        for j in range(4):\r\n","          for k in range(4):\r\n","            data.loc[index] = ([j, k, i, self.board[0][i][j][k]])\r\n","            index += 1\r\n","\r\n","      range_list=[-0.4,3.4]\r\n","      fig = px.scatter_3d(data,x=\"W\",y=\"D\",z=\"H\",color=\"Player\",\r\n","                          range_x=range_list,range_y=range_list,range_z=range_list,\r\n","                          color_discrete_map={0:\"rgba(0,0,0,0)\",1:\"red\",-1:\"blue\"},\r\n","                          opacity=0.95,width=854,height=480)\r\n","      fig.show()\r\n","  \r\n","  # 色が透明にならない問題あり\r\n","  def animation(self,obs_history):\r\n","    data = pd.DataFrame(index=[],columns=[\"W\",\"D\",\"H\",\"Player\",\"frame\"])\r\n","    index = 0\r\n","    dict_int_player={0:\"no one\",1:\"A\",-1:\"B\"}\r\n","    for frame in range(len(obs_history)):\r\n","      for i in range(4):\r\n","        for j in range(4):\r\n","          for k in range(4):\r\n","            data.loc[index] = ([j, k, i, obs_history[frame][i][j][k],frame])\r\n","            index += 1\r\n","\r\n","    range_list=[-0.4,3.4]\r\n","    fig = px.scatter_3d(data,x=\"W\",y=\"D\",z=\"H\",color=\"Player\",\r\n","                        animation_frame=\"frame\",\r\n","                        color_discrete_map={0:\"rgba(0,0,0,0)\",-1:\"red\",1:\"blue\"},\r\n","                        range_color=[-1,1],\r\n","                        range_x=range_list,range_y=range_list,range_z=range_list,\r\n","                        opacity=0.95,width=854,height=480)  \r\n","    fig.show()"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0h_RESMpSU4e"},"source":["## Agentの実装"]},{"cell_type":"code","metadata":{"id":"qYD5Ytm2TM3A","executionInfo":{"status":"ok","timestamp":1615262534582,"user_tz":-540,"elapsed":2463,"user":{"displayName":"kath mandu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijJDLCkA__3fFgGa09ZQUXu3DDTcT8QTwF2rBq=s64","userId":"12009895993958536903"}}},"source":["\"\"\"\n","   Prioritized Experience Replayを実現するためのメモリクラス.\n","\"\"\"\n","class PrioritizedReplayBuffer(object):\n","    def __init__(self, buffer_size):\n","        self.buffer_size = buffer_size\n","        self.index = 0\n","        self.buffer = []\n","        self.priorities = np.zeros(buffer_size, dtype=np.float32)\n","        self.priorities[0] = 1.0\n","    \n","    def __len__(self):\n","        return len(self.buffer)\n","\n","    # 経験をリプレイバッファに保存する． 経験は(obs, action, reward, next_obs, done)の5つ組を想定    \n","    def push(self, experience):\n","        if len(self.buffer) < self.buffer_size:\n","            self.buffer.append(experience)\n","        else:\n","            self.buffer[self.index] = experience\n","\n","        # 優先度は最初は大きな値で初期化しておき, 後でサンプルされた時に更新する\n","        self.priorities[self.index] = self.priorities.max()\n","        self.index = (self.index + 1) % self.buffer_size\n","    \n","    def sample(self, batch_size, alpha=0.6, beta=0.4):\n","        # 現在経験が入っている部分に対応する優先度を取り出し, サンプルする確率を計算\n","        priorities = self.priorities[: self.buffer_size if len(self.buffer) == self.buffer_size else self.index]\n","        priorities = priorities ** alpha\n","        prob = priorities / priorities.sum()\n","\n","        # サンプルする経験のインデックス\n","        indices = np.random.choice(len(self.buffer), batch_size, p=prob)\n","\n","        # 重点サンプリングの補正のための重みを計算\n","        weights = (len(self.buffer) * prob[indices])**(-beta)\n","        weights = weights / np.max(weights)\n","\n","        # 上でサンプルしたインデックスに基づいて経験をサンプルし, (obs, action, reward, next_obs, done)に分ける\n","        obs, action, reward, next_obs, done = zip(*[self.buffer[i] for i in indices])\n","\n","        # あとで計算しやすいようにtorch.Tensorに変換して(obs, action, reward, next_obs, done, indices, weights)の7つ組を返す\n","        return (torch.stack(obs),\n","                torch.as_tensor(action), \n","                torch.as_tensor(reward, dtype=torch.float32),\n","                torch.stack(next_obs), \n","                torch.as_tensor(done, dtype=torch.uint8),\n","                indices,\n","                torch.as_tensor(weights, dtype=torch.float32))\n","\n","    # 優先度を更新する. 優先度が極端に小さくなって経験が全く選ばれないということがないように, 微小値を加算しておく.\n","    def update_priorities(self, indices, priorities):\n","        self.priorities[indices] = priorities + 1e-4"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"MertMeXtUL3B","executionInfo":{"status":"ok","timestamp":1615262534582,"user_tz":-540,"elapsed":2461,"user":{"displayName":"kath mandu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijJDLCkA__3fFgGa09ZQUXu3DDTcT8QTwF2rBq=s64","userId":"12009895993958536903"}}},"source":["\"\"\"\n","    Dueling Networkを用いたQ関数を実現するためのニューラルネットワークをクラスとして記述します. \n","\"\"\"\n","class CNNQNetwork(nn.Module):\n","    def __init__(self, state_shape, n_action):\n","        super(CNNQNetwork, self).__init__()\n","        self.state_shape = state_shape\n","        self.n_action = n_action\n","        # Dueling Networkでも, 畳込み部分は共有する\n","        self.conv_layers = nn.Sequential(\n","            nn.Conv3d(state_shape[0], 32, kernel_size=2, stride=1),\n","            nn.ReLU(),\n","            nn.Conv3d(32, 64, kernel_size=2, stride=1),\n","            nn.ReLU(),\n","        )\n","\n","        cnn_out_size = self.check_cnn_size(state_shape) # CNNにかけた後の出力層の次元を解析\n","        \n","        # Dueling Networkのための分岐した全結合層\n","        # 状態価値\n","        self.fc_state = nn.Sequential(\n","            nn.Linear(cnn_out_size, 256),\n","            nn.ReLU(),\n","            nn.Linear(256, 1)\n","        )\n","\n","        # アドバンテージ\n","        self.fc_advantage = nn.Sequential(\n","            nn.Linear(cnn_out_size, 256),\n","            nn.ReLU(),\n","            nn.Linear(256, n_action)\n","        )\n","\n","    def check_cnn_size(self, shape):\n","        shape = torch.FloatTensor(1,shape[0],shape[1],shape[2],shape[3])\n","        out = self.conv_layers(shape).size()\n","        out = np.prod(np.array(out))\n","        return out\n","    \n","    def forward(self, obs):\n","        feature = self.conv_layers(obs)\n","        feature = feature.view(feature.size(0), -1)\n","\n","        state_values = self.fc_state(feature)\n","        advantage = self.fc_advantage(feature)\n","\n","        # 状態価値 + アドバンテージ で行動価値を計算しますが、安定化のためアドバンテージの（行動間での）平均を引きます\n","        action_values = state_values + advantage - torch.mean(advantage, dim=1, keepdim=True)\n","        return action_values\n","\n","    # epsilon-greedy. 確率epsilonでランダムに行動し, それ以外はニューラルネットワークの予測結果に基づいてgreedyに行動します. \n","    def act(self, obs, epsilon):\n","        if random.random() < epsilon:\n","            action = random.randrange(self.n_action)\n","        else:\n","            # 行動を選択する時には勾配を追跡する必要がない\n","            with torch.no_grad():\n","                action = torch.argmax(self.forward(obs.unsqueeze(0))).item()\n","        return action\n","    \n","    def act_greedy(self, obs):\n","      with torch.no_grad():\n","          action = torch.argmax(self.forward(obs.unsqueeze(0))).item()\n","      return action"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q8oCyRrfUVY5","executionInfo":{"status":"ok","timestamp":1615262534583,"user_tz":-540,"elapsed":2460,"user":{"displayName":"kath mandu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijJDLCkA__3fFgGa09ZQUXu3DDTcT8QTwF2rBq=s64","userId":"12009895993958536903"}}},"source":["def update(batch_size, beta):\n","    obs, action, reward, next_obs, done, indices, weights = replay_buffer.sample(batch_size, beta)\n","    obs, action, reward, next_obs, done, weights \\\n","        = obs.float().to(device), action.to(device), reward.to(device), next_obs.float().to(device), done.to(device), weights.to(device)\n","\n","    #　ニューラルネットワークによるQ関数の出力から, .gatherで実際に選択した行動に対応する価値を集めてきます.\n","    q_values = net(obs).gather(1, action.unsqueeze(1)).squeeze(1)\n","    \n","    \"\"\"\n","    print(\"action : \", action)\n","    print(\"net(obs) : \", net(obs))\n","    print(\"action.unsqueeze(1) : \", action.unsqueeze(1))\n","    print(\"net(obs).gather(1, action.unsqueeze(1)) : \", net(obs).gather(1, action.unsqueeze(1)))\n","    \"\"\"\n","\n","    # 目標値の計算なので勾配を追跡しない\n","    with torch.no_grad():\n","        # Double DQN. \n","        # ① 現在のQ関数でgreedyに行動を選択し, \n","        greedy_action_next = torch.argmax(net(next_obs), dim=1)\n","\n","        # ②　対応する価値はターゲットネットワークのものを参照します.\n","        q_values_next = target_net(next_obs).gather(1, greedy_action_next.unsqueeze(1)).squeeze(1)\n","\n","    # ベルマン方程式に基づき, 更新先の価値を計算します.\n","    # (1 - done)をかけているのは, ゲームが終わった後の価値は0とみなすためです.\n","    target_q_values = reward + gamma * q_values_next * (1 - done)\n","\n","    # Prioritized Experience Replayのために, ロスに重み付けを行なって更新します.\n","    optimizer.zero_grad()\n","    loss = (weights * loss_func(q_values, target_q_values)).mean()\n","    loss.backward()\n","    optimizer.step()\n","\n","    #　TD誤差に基づいて, サンプルされた経験の優先度を更新します.\n","    replay_buffer.update_priorities(indices, (target_q_values - q_values).abs().detach().cpu().numpy())\n","\n","    return loss.item()"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1KCJF0PMQtax"},"source":["## パラメータ"]},{"cell_type":"code","metadata":{"id":"nIFv8EXasjCb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615262534583,"user_tz":-540,"elapsed":2458,"user":{"displayName":"kath mandu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijJDLCkA__3fFgGa09ZQUXu3DDTcT8QTwF2rBq=s64","userId":"12009895993958536903"}},"outputId":"92d58cf1-b677-4d04-a26d-4cc9a6641629"},"source":["# Gym環境の定義\n","\n","num_grid = 4\n","num_win_seq = 4\n","win_reward=10\n","draw_penalty = 5\n","lose_penalty=10\n","couldnt_penalty = 1\n","time_penalty = 0.1\n","\n","\n","player_list = [-1,1]\n","first_player = player_list[random.randint(0,1)]\n","print(\"first_player is \",first_player)\n","\n","env = ScoreFour3dEnv(\n","  num_grid=num_grid,\n","  num_win_seq=num_win_seq, \n","  win_reward=win_reward, \n","  draw_penalty=draw_penalty,\n","  lose_penalty=lose_penalty, \n","  couldnt_penalty=couldnt_penalty, \n","  time_penalty=time_penalty, \n","  first_player=first_player\n",")\n"],"execution_count":11,"outputs":[{"output_type":"stream","text":["first_player is  1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"B_1qPjXyURoP","executionInfo":{"status":"ok","timestamp":1615262537592,"user_tz":-540,"elapsed":5466,"user":{"displayName":"kath mandu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijJDLCkA__3fFgGa09ZQUXu3DDTcT8QTwF2rBq=s64","userId":"12009895993958536903"}}},"source":["\"\"\"\n","    ハイパーパラメータ\n","\"\"\"\n","gamma = 0.99  #　割引率\n","batch_size = 32\n","n_episodes = 60000  # 学習を行うエピソード数\n","enemy_update_interval = 10000 # 敵のネットワークを更新する間隔(total_stepに依存)\n","\n","\n","\"\"\"\n","  SEED値\n","\"\"\"\n","SEED = 42\n","\n","\n","\n","\"\"\"\n","    リプレイバッファの宣言\n","\"\"\"\n","buffer_size = 100000  #　リプレイバッファに入る経験の最大数\n","initial_buffer_size = 10000  # 学習を開始する最低限の経験の数\n","replay_buffer = PrioritizedReplayBuffer(buffer_size)\n","\n","\n","\"\"\"\n","    ネットワークの宣言\n","\"\"\"\n","net = CNNQNetwork(env.observation_space.shape, n_action=env.action_space.n).to(device)\n","target_net = CNNQNetwork(env.observation_space.shape, n_action=env.action_space.n).to(device)\n","enemy_net = CNNQNetwork(env.observation_space.shape, n_action=env.action_space.n).to(device)\n","target_update_interval = 2000  # 学習安定化のために用いるターゲットネットワークの同期間隔(total_stepに依存)\n","\n","\n","\"\"\"\n","  ファインチューニング(事前に学習した重みを読み込む)\n","\"\"\"\n","load_weights_path=\"\"\n","\n","if load_weights_path != \"\":\n","  net.load_state_dict(torch.load(load_weights_path))\n","  target_net.load_state_dict(torch.load(load_weights_path))\n","  enemy_net.load_state_dict(torch.load(load_weights_path))\n","\n","\n","\"\"\"\n","    オプティマイザとロス関数の宣言\n","\"\"\"\n","optimizer = optim.Adam(net.parameters(), lr=1e-4)  # オプティマイザはAdam\n","loss_func = nn.SmoothL1Loss(reduction='none')  # ロスはSmoothL1loss（別名Huber loss）\n","\n","\n","\"\"\"\n","    Prioritized Experience Replayのためのパラメータβ(total_stepに比例)\n","\"\"\"\n","beta_begin = 0.4\n","beta_end = 1.0\n","beta_decay = n_episodes*12\n","# beta_beginから始めてbeta_endまでbeta_decayかけて線形に増やす\n","beta_func = lambda step: min(beta_end, beta_begin + (beta_end - beta_begin) * (step / beta_decay))\n","\n","\n","\"\"\"\n","    探索のためのパラメータε(total_stepに比例)\n","\"\"\"\n","epsilon_begin = 1.0\n","epsilon_end = 0.05\n","epsilon_decay = n_episodes*12\n","# epsilon_beginから始めてepsilon_endまでepsilon_decayかけて線形に減らす\n","epsilon_func = lambda step: max(epsilon_end, epsilon_begin - (epsilon_begin - epsilon_end) * (step / epsilon_decay))"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6MYas3233Q6f"},"source":["## 学習"]},{"cell_type":"code","metadata":{"id":"XnBB3exdUXr8","colab":{"base_uri":"https://localhost:8080/","height":69},"outputId":"d3528278-66ce-4727-db00-dbdc9c773e29"},"source":["# TensorBoardをColab内に起動\n","\n","tensorboard_path=os.path.join(each_dir,'logs')\n","%load_ext tensorboard\n","%tensorboard --logdir \"$tensorboard_path\" --port 9000"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The tensorboard extension is already loaded. To reload it, use:\n","  %reload_ext tensorboard\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/plain":["Launching TensorBoard..."]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"yJI1oieheawE","colab":{"base_uri":"https://localhost:8080/","height":624},"executionInfo":{"status":"error","timestamp":1615262576064,"user_tz":-540,"elapsed":43828,"user":{"displayName":"kath mandu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijJDLCkA__3fFgGa09ZQUXu3DDTcT8QTwF2rBq=s64","userId":"12009895993958536903"}},"outputId":"b7d4b63e-f58a-4d70-9c07-38e4f3345c37"},"source":["fix_seed(SEED)\n","env.seed(SEED)\n","env.action_space.seed(SEED)\n","\n","\n","JST = timezone(timedelta(hours=+9), 'JST')\n","now = datetime.now(JST).strftime('%Y%m%d-%H%M%S') \n","\n","log_path=os.path.join(each_dir,\"logs\",now)\n","weights_path=os.path.join(log_path,\"weights\")\n","os.makedirs(weights_path)\n","\n","writer = SummaryWriter(log_path)\n","\n","# 記録用にコピーを作成\n","shutil.copyfile(os.path.join(each_dir,\"ScoreFour.ipynb\"),os.path.join(log_path,\"for_record.ipynb\")) # each_dirの直下にScoreFour.ipynbという名前で置くこと\n","os.chmod(os.path.join(log_path,\"for_record.ipynb\"),SF_IMMUTABLE)\n","\n","\n","info={\"turn\": first_player, \"winner\": 0}\n","\n","AGENT_TURN =  1\n","ENEMY_TURN = -1\n","\n","total_step = 0\n","total_reward = 0\n","enemy_update = 0\n","\n","win_num=0\n","lose_num=0\n","draw_num=0\n","\n","for episode in range(n_episodes):\n","  obs = env.reset()\n","  done = False\n","\n","  start_step = total_step\n","\n","  episode_reward=0\n","\n","  # for animation\n","  episode_cube_history = []\n","  episode_cube_history.append(np.array(obs.squeeze(0)))\n","\n","  episode_couldnt_locate_num=0\n","\n","  while not done:\n","    sum_reward = 0\n","    step_done=0\n","\n","    while step_done != 2:\n","      if (info[\"turn\"] == AGENT_TURN):\n","        before_action_obs=obs\n","        player_action = net.act(obs.float().to(device), epsilon_func(total_step)) # ε-greedyで行動を選択\n","        next_obs, player_reward, done, info = env.step(player_action) # 環境中で実際に行動\n","        after_action_obs = next_obs\n","\n","        if info[\"is_couldnt_locate\"]==True:\n","          # print(\"agent couldn't locate\")\n","          episode_reward += player_reward\n","          total_reward += player_reward\n","          replay_buffer.push([before_action_obs, player_action, player_reward, after_action_obs, done])# 置けなかったときのことを学習させる\n","          episode_couldnt_locate_num += 1\n","          pass\n","        else:\n","          step_done+=1\n","          sum_reward += player_reward\n","\n","        if done :\n","          break\n","\n","      elif (info[\"turn\"] == ENEMY_TURN):\n","        if False:  # enemy_update > 20:\n","          enemy_action = enemy_net.act_greedy(obs.float().to(device)) # 相手はgreedy方策で行動選択\n","        else:\n","          enemy_action = env.action_space.sample() # 最初はランダム\n","        next_obs, enemy_reward, done, info = env.step(enemy_action) # 環境中で実際に行動\n","        if info[\"is_couldnt_locate\"]==True:\n","          # print(\"enemy couldn't locate\")\n","          pass\n","        else:\n","          step_done+=1\n","        \n","        if (done):# 相手のcouldnt_penaltyをsum_rewardに入れないように\n","          sum_reward -= enemy_reward # 相手が勝利して得た報酬を引く\n","          break\n","\n","      obs = next_obs\n","      if info[\"is_couldnt_locate\"]==False:\n","        episode_cube_history.append(np.array(obs.squeeze(0))) # for animation\n","\n","    # print(sum_reward)\n","\n","    # リプレイバッファに経験を蓄積 (これで大丈夫？)\n","    replay_buffer.push([before_action_obs, player_action, sum_reward, after_action_obs, done])\n","\n","    total_step += 1 # stepはAgentがactionを実行した回数とするため、for文中に入れない\n","\n","    episode_reward += sum_reward\n","    total_reward += sum_reward\n","\n","    \n","    # ネットワークを更新\n","    if len(replay_buffer) > initial_buffer_size:\n","      loss = update(batch_size, beta_func(total_step))\n","      writer.add_scalar('Loss', loss, total_step)\n","    \n","    # enemyネットワークを定期的に強くする\n","    if (total_step + 1) % enemy_update_interval == 0:\n","        enemy_net.load_state_dict(target_net.state_dict())\n","        enemy_update += 1\n","\n","    # ターゲットネットワークを定期的に同期させる\n","    if (total_step + 1) % target_update_interval == 0:\n","        target_net.load_state_dict(net.state_dict())\n","\n","    if done:\n","      if (info[\"winner\"] == AGENT_TURN):\n","        # print(\"Win!!!\")\n","        win_num+=1\n","      elif (info[\"winner\"] == ENEMY_TURN):\n","        # print(\"Lose...\")\n","        lose_num+=1\n","      else:\n","        # print(\"Draw\")\n","        draw_num+=1\n","  \n","  episode_step = total_step-start_step\n","\n","  if((episode+1) % 50 == 0):\n","    print('Episode: {},  TotalStep: {}, EpisodeStep: {},  EpisodeReward: {}'.format(episode + 1, total_step,episode_step, episode_reward))\n","\n","  writer.add_scalar('Total-Reward', total_reward, episode)\n","  writer.add_scalar('Episode-Reward', episode_reward, episode)\n","  writer.add_scalar('Episode-Step', episode_step, episode)\n","  writer.add_scalar('Win-Rate', win_num/(episode+1)*100, episode) \n","  writer.add_scalar('Draw-Rate', draw_num/(episode+1)*100, episode) \n","  writer.add_scalar('Lose-Rate', lose_num/(episode+1)*100, episode) \n","  writer.add_scalar('Episode-Couldnt-Locate-Num', episode_couldnt_locate_num, episode)\n","\n","  if((episode+1) % 2000 == 0):\n","    torch.save(net.state_dict(), weights_path+\"/weights_{}episodes.pth\".format(episode+1))\n","\n","  # env.render(mode=\"plot\", isClear=False)\n","  # env.animation(episode_cube_history)\n","torch.save(net.state_dict(), weights_path+\"/weights_final.pth\")"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Episode: 50,  TotalStep: 725, EpisodeStep: 15,  EpisodeReward: -10\n","Episode: 100,  TotalStep: 1463, EpisodeStep: 14,  EpisodeReward: -11\n","Episode: 150,  TotalStep: 2159, EpisodeStep: 10,  EpisodeReward: 10\n","Episode: 200,  TotalStep: 2862, EpisodeStep: 27,  EpisodeReward: -22\n","Episode: 250,  TotalStep: 3642, EpisodeStep: 10,  EpisodeReward: -10\n","Episode: 300,  TotalStep: 4353, EpisodeStep: 10,  EpisodeReward: -10\n","Episode: 350,  TotalStep: 5096, EpisodeStep: 12,  EpisodeReward: 10\n","Episode: 400,  TotalStep: 5842, EpisodeStep: 20,  EpisodeReward: -11\n","Episode: 450,  TotalStep: 6559, EpisodeStep: 10,  EpisodeReward: 10\n","Episode: 500,  TotalStep: 7345, EpisodeStep: 8,  EpisodeReward: -10\n","Episode: 550,  TotalStep: 8108, EpisodeStep: 15,  EpisodeReward: -10\n","Episode: 600,  TotalStep: 8755, EpisodeStep: 12,  EpisodeReward: -10\n","Episode: 650,  TotalStep: 9476, EpisodeStep: 13,  EpisodeReward: -10\n","Episode: 700,  TotalStep: 10205, EpisodeStep: 17,  EpisodeReward: 9\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-7ba81064dd62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;31m# ネットワークを更新\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0minitial_buffer_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m       \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-10-800e6792696e>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(batch_size, beta)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_q_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"soTLuxKgJVdb"},"source":["## 学習結果の確認"]},{"cell_type":"code","metadata":{"id":"UPuwven5XxDH","executionInfo":{"status":"aborted","timestamp":1615262576064,"user_tz":-540,"elapsed":43826,"user":{"displayName":"kath mandu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijJDLCkA__3fFgGa09ZQUXu3DDTcT8QTwF2rBq=s64","userId":"12009895993958536903"}}},"source":["obs = env.reset()\r\n","log = []\r\n","log_child = []\r\n","num_done = 0\r\n","info={\"turn\": first_player, \"winner\": 0}\r\n","\r\n","while(num_done < 5):  # 5回分のデータをとる\r\n","    if(info[\"turn\"]==1):\r\n","      action = net.act(obs.to(device), epsilon=0.05)\r\n","      obs, reward, done, info = env.step(action)\r\n","      log_child.append([reward, info])\r\n","\r\n","    elif(info[\"turn\"]==-1):\r\n","      action = env.action_space.sample()\r\n","      obs, reward, done, info = env.step(action)\r\n","    \r\n","\r\n","    if done:\r\n","        if(info[\"winner\"]==1):\r\n","          print(\"Win!!!\")\r\n","        elif(info[\"winner\"]==-1):\r\n","          print(\"Lose...\")\r\n","        else:\r\n","          print(\"Draw\")\r\n","\r\n","        env.render(mode=\"plot\",isClear=False)\r\n","        obs = env.reset()\r\n","        log.append(log_child)\r\n","        log_child = []\r\n","        num_done += 1\r\n","        print()\r\n","display(log)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f1Wlorc9TBc1"},"source":["## 研究メモ"]},{"cell_type":"markdown","metadata":{"id":"8XJsC4jyU4wg"},"source":["### エラー対処\r\n","- element 0 of tensors does not require grad and does not have a grad_fn\r\n","  - Reset Runtime"]},{"cell_type":"markdown","metadata":{"id":"P4DtnfhwTP-_"},"source":["### History (Manato)\r\n","\r\n","#### 2021/03/01\r\n","- チーム結成\r\n","\r\n","#### 2021/03/04\r\n","- テーマ決定\r\n","- 可視化が出来ることを確認\r\n","- ScoreFour実装\r\n","\r\n","#### 2021/03/05\r\n","- バグ修正\r\n","- OpenAIGymのAPI形式に合わせて実装\r\n","\r\n","#### 2021/03/06\r\n","- kubotaniさんが超絶リファクタリング\r\n","- 1,000,000エピソードで学習\r\n","  - 実装がうまくいっていなかったため中断\r\n","- kubotaniさんのコードのバグを修正\r\n","\r\n","#### 2021/03/07\r\n","- 負けた時の報酬をどのように与えるかがタスク\r\n","\r\n","#### 2021/03/08\r\n","- ミーティング\r\n","- template 作成"]}]}