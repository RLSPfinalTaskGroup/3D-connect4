{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ScoreFour.ipynb","provenance":[{"file_id":"1JFgYobx-m0cqTRZwRv-uEN5m-zJG-iQ9","timestamp":1614992719457},{"file_id":"1jZyoHU8SFD0rF04k8nsu-lKJLiiAv_t7","timestamp":1614962802098}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"favOHMTeI8ZY"},"source":["# 強化学習を用いた3D Score Fourの攻略"]},{"cell_type":"markdown","metadata":{"id":"Di5CgaB0jF92"},"source":["変更するべきパラメータ\n","\n","- Gym環境の報酬\n","- Network\n","- ReplayBufferに学習させる\n","e.t.c"]},{"cell_type":"markdown","metadata":{"id":"lKXC26EvI8jL"},"source":["## 各種設定"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IxVJPsJEVPP7","outputId":"805dd899-065b-4f5a-f0fd-aeaa96422c8f"},"source":["# Driveのマウント(logをドライブに保存)\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.activity.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fexperimentsandconfigs%20https%3a%2f%2fwww.googleapis.com%2fauth%2fphotos.native&response_type=code\n","\n","Enter your authorization code:\n","4/1AY0e-g4kwffX53wvZho96B-jCt1_khrqDb_GCpkQ1oyiqPZ394Ke8ggv9kg\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"x4w_pfd1-T2m"},"source":["# 各々のフォルダ\n","each_dir = \"/content/drive/MyDrive/Colab Notebooks/MatsuoSeminer/Research\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4axWZO3Tk8zj"},"source":["# ライブラリのインポート\n","\n","import gym\n","from gym import error, spaces, utils\n","from gym.utils import seeding\n","\n","import plotly.express as px\n","import pandas as pd\n","import numpy as np\n","from google.colab import output\n","import random\n","import time\n","\n","from typing import List, Tuple, Union, Iterable\n","\n","import torch\n","from torch import nn, optim\n","from torch.utils.tensorboard import SummaryWriter\n","\n","from datetime import datetime, timedelta, timezone\n","import os\n","from stat import SF_IMMUTABLE\n","\n","import shutil\n","\n","from tqdm.notebook import tqdm\n","\n","!pip install pytorch-summary\n","import torchsummary\n","\n","%load_ext tensorboard"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RcY4SgSSs-Dl"},"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PsSy-ze4tBt7"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kzlClBZzcyXe"},"source":["# Seed値の固定\n","\n","def fix_seed(seed):\n","    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n","    # random\n","    random.seed(seed)\n","    # Numpy\n","    np.random.seed(seed)\n","    # Pytorch\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False # 処理速度は落ちる\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZHtB9b8gk4G_"},"source":["## 立体四目並べ OpenAIGym形式"]},{"cell_type":"markdown","metadata":{"id":"eDZ7BRqIbW0V"},"source":["cubeとboardの違い\n","- cube.shape = (self.num_grid,self.num_grid,self.num_grid)  \n","- board.shape = (1,self.num_grid,self.num_grid,self.num_grid)\n","\n","初期化時の引数\n","\n","|引数|内容|  \n","|:--------:|:--------:|\n","|num_grid|一辺の長さ|\n","|num_win_seq|勝利条件(この数一列に並んだら勝利)|\n","|win_reward|勝利時の報酬|\n","|draw_penalty|引き分け時のペナルティ|\n","|lose_penalty|敗北時のペナルティ|\n","|couldnt_locate_penalty|おけない場所を選択した時のペナルティ|\n","|time_penalty|使用していない|\n","|first_player|先攻|\n","\n","- 報酬・ペナルティについては全て正の値で設定すること\n","- ペナルティは-1を掛けた値を足します"]},{"cell_type":"code","metadata":{"id":"0rf7pMG3lHQj"},"source":["class ScoreFour3dEnv(gym.Env):\n","  def __init__(self, num_grid=4, num_win_seq=4, win_reward=10, draw_penalty=5,lose_penalty=10,could_locate_reward=0.1, couldnt_locate_penalty=0.1, time_penalty=0.1, first_player=1):\n","    super().__init__()\n","\n","    self.num_grid = num_grid\n","    self.num_win_seq = num_win_seq\n","    self.win_reward = win_reward\n","    self.draw_penalty = draw_penalty\n","    self.lose_penalty = lose_penalty # 今のところ使用しない\n","    self.could_locate_reward = could_locate_reward\n","    self.couldnt_locate_penalty = couldnt_locate_penalty\n","    self.time_penalty = time_penalty\n","    \n","    # アクションの数の設定\n","    self.action_space = gym.spaces.Discrete(self.num_grid*self.num_grid)\n","    # 観測空間(state)を定義\n","    self.observation_space = gym.spaces.Box(low=-1, high=1, shape=(1, self.num_grid, self.num_grid, self.num_grid))\n","\n","    # 最初のプレーヤーがどちらかを定義\n","    self.player = first_player\n","\n","    # 判定用定数\n","    self.WIN_A = np.full(num_win_seq,1)\n","    self.WIN_B = np.full(num_win_seq,-1)\n","\n","    self.reset()\n","\n","  def reset(self):\n","    self.board = [[[[0]*self.num_grid for _ in range(self.num_grid)] for _ in range(self.num_grid)]]\n","    return torch.tensor(self.board).float()\n","\n","  def step(self, action):\n","    action = self.base_change(action, self.num_grid).zfill(2)\n","    W = int(action[0])\n","    D = int(action[1])\n","    reward = 0\n","    winner = 0\n","    done = False\n","    is_couldnt_locate=False\n","\n","\n","    for H in range(self.num_grid):\n","      if (self.board[0][H][W][D]==0): # 空いていたら置く\n","        self.board[0][H][W][D] = self.player\n","        self.player *= -1\n","        reward = self.could_locate_reward\n","        break\n","      elif (H == self.num_grid-1):\n","        # print(\"Couldn't Locate!!\")\n","        reward = -self.couldnt_locate_penalty\n","        is_couldnt_locate=True\n","      else:\n","        pass\n","    \n","    done = self.is_game_end(np.array(self.board[0]))\n","\n","    if (done): # stepを施行した側は勝つ以外ありえない\n","      reward = self.win_reward\n","      winner = self.player*-1\n","    elif (not(0 in np.array(self.board[0]).flatten())): # draw\n","      done = True\n","      reward = -self.draw_penalty\n","    else:\n","      pass\n","\n","    info={\"turn\": self.player, \"winner\": winner, \"is_couldnt_locate\":is_couldnt_locate}\n","\n","    return torch.tensor(self.board).float(), reward, done, info\n","  \n","  # 入力をbaseで指定した進数に変換\n","  def base_change(self, value, base):\n","    if (int(value / base)):\n","      return self.base_change(int(value / base), base) + str(value % base)\n","    return str(value % base)\n","\n","\n","  def is_game_end(self, cube: np.ndarray) -> bool:\n","    num_stride = self.num_grid - self.num_win_seq + 1\n","\n","    # 1辺self.num_grudマスの格子内で、1辺self.num_win_seqマスのcubeを1マスずつずらしていく\n","    for dim_H_stride_id in range(num_stride):\n","      for dim_W_stride_id in range(num_stride):\n","        for dim_D_stride_id in range(num_stride):\n","          search_cube = cube[dim_H_stride_id:dim_H_stride_id+self.num_win_seq,\n","                        dim_W_stride_id:dim_W_stride_id+self.num_win_seq,\n","                        dim_D_stride_id:dim_D_stride_id+self.num_win_seq]\n","          \n","          # x,y,z軸各方向に垂直な面について解析\n","          cube_list = [search_cube, np.rot90(search_cube,axes=(0, 2)), np.rot90(search_cube,axes=(1, 2))] \n","\n","          # cube内の考えうる全ての二次元平面上でループ\n","          for each_cube in cube_list:\n","            for i in range(self.num_win_seq):\n","              # 2次元平面上でビンゴしていないか確認\n","              if self.is_end_on_2d_plane(each_cube[i]):\n","                return True\n","              if self.is_end_on_2d_plane(each_cube[i].T):\n","                return True\n","\n","          # 立体的な斜め\n","          for i in range(4):\n","            cube = np.rot90(cube)\n","            if (self.is_diag_on_3d_cube(cube)):\n","              return True\n","    \n","    return False\n","\n","\n","  # N×Nの2次元配列上でN個玉が並んでいるところがあるかを判定する関数。（ビンゴの判定みたいなもの）\n","  def is_end_on_2d_plane(self, plane: np.ndarray) -> bool:\n","    assert plane.shape == (self.num_win_seq, self.num_win_seq)\n","\n","    # 行\n","    for row in plane:\n","      if(all(row == self.WIN_A)):\n","        return True\n","      elif(all(row == self.WIN_B)):\n","        return True\n","    \n","    # 斜め(片側)\n","    oblique_elements = np.empty(0)\n","    for a in range(self.num_win_seq):\n","      for b in range(self.num_win_seq):\n","        if(a==b):\n","          oblique_elements = np.append(oblique_elements,plane[a][b])\n","\n","    if(all(oblique_elements == self.WIN_A)):\n","      return True\n","    elif(all(oblique_elements == self.WIN_B)):\n","      return True\n","\n","    return False\n","\n","\n","  # N×N×Nの3次元配列上で、N個の玉が立体対角上に並んでいるかどうかを判定する関数。\n","  def is_diag_on_3d_cube(self, cube: np.ndarray) -> bool:\n","    assert cube.shape == (self.num_win_seq, self.num_win_seq, self.num_win_seq)\n","\n","    oblique_elements = np.empty(0)\n","    for f in range(self.num_win_seq):\n","      for a in range(self.num_win_seq):\n","        for b in range(self.num_win_seq):\n","          if(f==a and a==b and f==b):\n","            oblique_elements = np.append(oblique_elements,cube[f][a][b])\n","\n","    if(all(oblique_elements == np.full(self.num_win_seq,1))):\n","      return True\n","    elif(all(oblique_elements == np.full(self.num_win_seq,-1))):\n","      return True\n","    return False\n","\n","\n","\n","  def render(self, mode = \"print\", isClear = False):\n","    if (isClear):\n","      output.clear() #出力の消去\n","    \n","    if (mode == \"print\"):\n","      i = 0\n","      for square in self.board[0]:\n","        print(\"{}F\".format(i))\n","        for line in square:\n","          print(line)\n","        i += 1\n","    \n","    elif (mode == \"plot\"):\n","      data = pd.DataFrame(index=[],columns=[\"W\",\"D\",\"H\",\"Player\"])\n","      index = 0\n","      for i in range(self.num_grid):\n","        for j in range(self.num_grid):\n","          for k in range(self.num_grid):\n","            data.loc[index] = ([j, k, i, self.board[0][i][j][k]])\n","            index += 1\n","\n","      range_list=[-0.4,self.num_grid-0.6]\n","      fig = px.scatter_3d(data,x=\"W\",y=\"D\",z=\"H\",color=\"Player\",\n","                          range_x=range_list,range_y=range_list,range_z=range_list,\n","                          color_discrete_map={0:\"rgba(0,0,0,0)\",1:\"red\",-1:\"blue\"},\n","                          opacity=0.95,width=854,height=480)\n","      fig.show()\n","  \n","  # 色が透明にならない問題あり\n","  def animation(self,obs_history):\n","    data = pd.DataFrame(index=[],columns=[\"W\",\"D\",\"H\",\"Player\",\"frame\"])\n","    index = 0\n","    dict_int_player={0:\"no one\",1:\"A\",-1:\"B\"}\n","    for frame in range(len(obs_history)):\n","      for i in range(self.num_grid):\n","        for j in range(self.num_grid):\n","          for k in range(self.num_grid):\n","            data.loc[index] = ([j, k, i, obs_history[frame][i][j][k],frame])\n","            index += 1\n","\n","    range_list=[-0.4,self.num_grid-0.6]\n","    fig = px.scatter_3d(data,x=\"W\",y=\"D\",z=\"H\",color=\"Player\",\n","                        animation_frame=\"frame\",\n","                        color_discrete_map={0:\"rgba(0,0,0,0)\",-1:\"red\",1:\"blue\"},\n","                        range_color=[-1,1],\n","                        range_x=range_list,range_y=range_list,range_z=range_list,\n","                        opacity=0.95,width=854,height=480)  \n","    fig.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QecTUxgAa0YP"},"source":["def validate_random(net,experiment_times=20,first_player=1):\n","  win_num=0\n","  lose_num=0\n","  draw_num=0\n","\n","  total_step=0\n","  total_couldnt_step=0\n","  \n","  info={\"turn\": first_player, \"winner\": 0}\n","  for i in range(experiment_times):\n","    done=False\n","    obs = env.reset()\n","\n","    while not done:\n","      sum_reward = 0\n","      step_done=0\n","      isFirstCouldntLocate=False\n","      while step_done != 2:\n","        if (info[\"turn\"] == AGENT_TURN):\n","          if not isFirstCouldntLocate: \n","            total_step+=1\n","            player_action = net.act(obs.float().to(device), 0.05) \n","            next_obs, player_reward, done, info = env.step(player_action) # 環境中で実際に行動\n","            if info[\"is_couldnt_locate\"]==True:\n","              total_couldnt_step+=1\n","              isFirstCouldntLocate=True\n","              pass\n","            else:\n","              step_done+=1\n","              sum_reward += player_reward\n","          else:\n","            player_action = env.action_space.sample()\n","            next_obs, player_reward, done, info = env.step(player_action) # 環境中で実際に行動\n","            if info[\"is_couldnt_locate\"]==True:\n","              pass\n","            else:\n","              step_done+=1\n","              sum_reward += 0\n","\n","          if done :\n","            break\n","\n","        elif (info[\"turn\"] == ENEMY_TURN):\n","          enemy_action = env.action_space.sample() # ランダム\n","          next_obs, enemy_reward, done, info = env.step(enemy_action) # 環境中で実際に行動\n","          if info[\"is_couldnt_locate\"]==True:\n","            pass\n","          else:\n","            step_done+=1\n","          \n","          if (done):# 相手のcouldnt_locate_penaltyとcould_locate_rewardをsum_rewardに入れないように\n","            sum_reward -= enemy_reward # 相手が勝利して得た報酬を引く       \n","            break\n","\n","        obs = next_obs\n","\n","    if(info[\"winner\"]==1):\n","      win_num+=1\n","    elif(info[\"winner\"]==-1):\n","      lose_num+=1\n","    else:\n","      draw_num+=1\n","\n","  win_rate=win_num/(experiment_times)*100\n","  draw_rate=draw_num/(experiment_times)*100\n","  lose_rate=lose_num/(experiment_times)*100\n","  couldnt_rate=total_couldnt_step/(total_step)*100\n","  return win_rate, draw_rate, lose_rate,couldnt_rate\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0h_RESMpSU4e"},"source":["## Agentの実装"]},{"cell_type":"code","metadata":{"id":"qYD5Ytm2TM3A"},"source":["\"\"\"\n","   Prioritized Experience Replayを実現するためのメモリクラス.\n","\"\"\"\n","class PrioritizedReplayBuffer(object):\n","    def __init__(self, buffer_size):\n","        self.buffer_size = buffer_size\n","        self.index = 0\n","        self.buffer = []\n","        self.priorities = np.zeros(buffer_size, dtype=np.float32)\n","        self.priorities[0] = 1.0\n","    \n","    def __len__(self):\n","        return len(self.buffer)\n","\n","    # 経験をリプレイバッファに保存する． 経験は(obs, action, reward, next_obs, done)の5つ組を想定    \n","    def push(self, experience):\n","        if len(self.buffer) < self.buffer_size:\n","            self.buffer.append(experience)\n","        else:\n","            self.buffer[self.index] = experience\n","\n","        # 優先度は最初は大きな値で初期化しておき, 後でサンプルされた時に更新する\n","        self.priorities[self.index] = self.priorities.max()\n","        self.index = (self.index + 1) % self.buffer_size\n","    \n","    def sample(self, batch_size, alpha=0.6, beta=0.4):\n","        # 現在経験が入っている部分に対応する優先度を取り出し, サンプルする確率を計算\n","        priorities = self.priorities[: self.buffer_size if len(self.buffer) == self.buffer_size else self.index]\n","        priorities = priorities ** alpha\n","        prob = priorities / priorities.sum()\n","\n","        # サンプルする経験のインデックス\n","        indices = np.random.choice(len(self.buffer), batch_size, p=prob)\n","\n","        # 重点サンプリングの補正のための重みを計算\n","        weights = (len(self.buffer) * prob[indices])**(-beta)\n","        weights = weights / np.max(weights)\n","\n","        # 上でサンプルしたインデックスに基づいて経験をサンプルし, (obs, action, reward, next_obs, done)に分ける\n","        obs, action, reward, next_obs, done = zip(*[self.buffer[i] for i in indices])\n","\n","        # あとで計算しやすいようにtorch.Tensorに変換して(obs, action, reward, next_obs, done, indices, weights)の7つ組を返す\n","        return (torch.stack(obs),\n","                torch.as_tensor(action), \n","                torch.as_tensor(reward, dtype=torch.float32),\n","                torch.stack(next_obs), \n","                torch.as_tensor(done, dtype=torch.uint8),\n","                indices,\n","                torch.as_tensor(weights, dtype=torch.float32))\n","\n","    # 優先度を更新する. 優先度が極端に小さくなって経験が全く選ばれないということがないように, 微小値を加算しておく.\n","    def update_priorities(self, indices, priorities):\n","        self.priorities[indices] = priorities + 1e-4"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MertMeXtUL3B"},"source":["\"\"\"\n","    Dueling Networkを用いたQ関数を実現するためのニューラルネットワークをクラスとして記述します. \n","\"\"\"\n","class CNNQNetwork(nn.Module):\n","    def __init__(self, state_shape, n_action):\n","        super(CNNQNetwork, self).__init__()\n","        self.state_shape = state_shape\n","        self.n_action = n_action\n","        # Dueling Networkでも, 畳込み部分は共有する\n","        self.conv_layers = nn.Sequential(\n","            #nn.Conv3d(state_shape[0],64,kernel_size=2),\n","            #nn.LeakyReLU()\n","            nn.Flatten(),\n","            nn.Linear(state_shape[1]*state_shape[2]*state_shape[3],512),\n","            nn.LeakyReLU(),\n","            nn.LayerNorm(512),\n","            nn.Linear(512, 512),\n","            nn.LeakyReLU(),\n","        )\n","\n","        cnn_out_size = self.check_cnn_size(state_shape) # CNNにかけた後の出力層の次元を解析\n","\n","        # Dueling Networkのための分岐した全結合層\n","        # 状態価値\n","        self.fc_state = nn.Sequential(\n","            nn.Linear(cnn_out_size, 512),\n","            nn.LeakyReLU(),\n","            nn.Linear(512, 256),\n","            nn.LeakyReLU(),\n","            nn.Linear(256, 64),\n","            nn.LeakyReLU(),\n","            nn.Linear(64, 1)\n","        )\n","\n","        # アドバンテージ\n","        self.fc_advantage = nn.Sequential(\n","            nn.Linear(cnn_out_size, 512),\n","            nn.LeakyReLU(),\n","            nn.Linear(512, 256),\n","            nn.LeakyReLU(),\n","            nn.Linear(256, 64),\n","            nn.LeakyReLU(),\n","            nn.Linear(64, n_action)\n","        )\n","\n","    def check_cnn_size(self, shape):\n","        shape = torch.FloatTensor(1,shape[0],shape[1],shape[2],shape[3])\n","        out = self.conv_layers(shape).size()\n","        out = np.prod(np.array(out))\n","        return out\n","    \n","    def forward(self, obs):\n","        feature = self.conv_layers(obs)\n","        feature = feature.view(feature.size(0), -1)\n","\n","        state_values = self.fc_state(feature)\n","        advantage = self.fc_advantage(feature)\n","\n","        # 状態価値 + アドバンテージ で行動価値を計算しますが、安定化のためアドバンテージの（行動間での）平均を引きます\n","        action_values = state_values + advantage - torch.mean(advantage, dim=1, keepdim=True)\n","        return action_values\n","\n","    # epsilon-greedy. 確率epsilonでランダムに行動し, それ以外はニューラルネットワークの予測結果に基づいてgreedyに行動します. \n","    def act(self, obs, epsilon):\n","        if random.random() < epsilon:\n","            action = random.randrange(self.n_action)\n","        else:\n","            # 行動を選択する時には勾配を追跡する必要がない\n","            with torch.no_grad():\n","                action = torch.argmax(self.forward(obs.unsqueeze(0))).item()\n","        return action\n","    \n","    def act_greedy(self, obs):\n","      with torch.no_grad():\n","          action = torch.argmax(self.forward(obs.unsqueeze(0))).item()\n","      return action"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q8oCyRrfUVY5"},"source":["def update(batch_size, beta):\n","    obs, action, reward, next_obs, done, indices, weights = replay_buffer.sample(batch_size, beta)\n","    obs, action, reward, next_obs, done, weights \\\n","        = obs.float().to(device), action.to(device), reward.to(device), next_obs.float().to(device), done.to(device), weights.to(device)\n","\n","    #　ニューラルネットワークによるQ関数の出力から, .gatherで実際に選択した行動に対応する価値を集めてきます.\n","    q_values = net(obs).gather(1, action.unsqueeze(1)).squeeze(1)\n","    \n","    \"\"\"\n","    print(\"action : \", action)\n","    print(\"net(obs) : \", net(obs))\n","    print(\"action.unsqueeze(1) : \", action.unsqueeze(1))\n","    print(\"net(obs).gather(1, action.unsqueeze(1)) : \", net(obs).gather(1, action.unsqueeze(1)))\n","    \"\"\"\n","\n","    # 目標値の計算なので勾配を追跡しない\n","    with torch.no_grad():\n","        # Double DQN. \n","        # ① 現在のQ関数でgreedyに行動を選択し, \n","        greedy_action_next = torch.argmax(net(next_obs), dim=1)\n","\n","        # ②　対応する価値はターゲットネットワークのものを参照します.\n","        q_values_next = target_net(next_obs).gather(1, greedy_action_next.unsqueeze(1)).squeeze(1)\n","\n","    # ベルマン方程式に基づき, 更新先の価値を計算します.\n","    # (1 - done)をかけているのは, ゲームが終わった後の価値は0とみなすためです.\n","    target_q_values = reward + gamma * q_values_next * (1 - done)\n","\n","    # Prioritized Experience Replayのために, ロスに重み付けを行なって更新します.\n","    optimizer.zero_grad()\n","    loss = (weights * loss_func(q_values, target_q_values)).mean()\n","    loss.backward()\n","    optimizer.step()\n","\n","    #　TD誤差に基づいて, サンプルされた経験の優先度を更新します.\n","    replay_buffer.update_priorities(indices, (target_q_values - q_values).abs().detach().cpu().numpy())\n","\n","    return loss.item()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1KCJF0PMQtax"},"source":["## パラメータ"]},{"cell_type":"code","metadata":{"id":"nIFv8EXasjCb"},"source":["# Gym環境の定義\n","\n","num_grid = 4\n","num_win_seq = 4\n","win_reward = 10\n","draw_penalty = 5\n","lose_penalty = 10\n","could_locate_reward = 0.01\n","couldnt_locate_penalty = 0.2\n","time_penalty = 0.1\n","\n","def make_game_env():\n","  player_list = [-1,1]\n","  first_player = player_list[random.randint(0,1)]\n","  print(\"first_player is \",first_player)\n","\n","  env = ScoreFour3dEnv(\n","    num_grid=num_grid,\n","    num_win_seq=num_win_seq, \n","    win_reward=win_reward, \n","    draw_penalty=draw_penalty,\n","    lose_penalty=lose_penalty, \n","    could_locate_reward=could_locate_reward,\n","    couldnt_locate_penalty=couldnt_locate_penalty, \n","    time_penalty=time_penalty, \n","    first_player=first_player\n","  )\n","  return first_player,env\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B_1qPjXyURoP"},"source":["\"\"\"\n","    ハイパーパラメータ\n","\"\"\"\n","gamma = 0.99  #　割引率\n","batch_size = 1\n","n_episodes = 100000  # 学習を行うエピソード数\n","enemy_update_interval = 1000 # 敵のネットワークを更新する間隔(episodeに依存)\n","\n","\n","\"\"\"\n","  SEED値\n","\"\"\"\n","SEED = 7\n","\n","\n","\"\"\"\n","    リプレイバッファの宣言\n","\"\"\"\n","buffer_size = 200000  #　リプレイバッファに入る経験の最大数\n","initial_buffer_size = 100  # 学習を開始する最低限の経験の数\n","\n","\n","\"\"\"\n","    ネットワークの宣言\n","\"\"\"\n","# SEED値の関係で後述\n","target_update_interval = 200  # 学習安定化のために用いるターゲットネットワークの同期間隔(episodeに依存)\n","net_save_interval = 4000 # networkの重みを保存する間隔(episode依存)\n","\n","\n","\"\"\"\n","  ファインチューニング(事前に学習した重みを読み込む)\n","\"\"\"\n","do_fine_tuning = False\n","time = \"20210312-141214\"\n","episode = 10000\n","episode = \"weights_{}episodes.pth\".format(episode)\n","load_weights_path = os.path.join(each_dir,\"logs\",time,\"weights\",episode)\n","\n","\n","\"\"\"\n","    ロス関数の宣言\n","\"\"\"\n","loss_func = nn.SmoothL1Loss(reduction='none')  # ロスはSmoothL1loss（別名Huber loss）\n","# オプティマイザーはSEEDの関係で後述\n","\n","\n","\"\"\"\n","    Prioritized Experience Replayのためのパラメータβ(episodeに依存)\n","\"\"\"\n","beta_begin = 0.2\n","beta_end = 0.95\n","beta_decay = n_episodes - 2000\n","# beta_beginから始めてbeta_endまでbeta_decayかけて線形に増やす\n","beta_func = lambda episode: min(beta_end, beta_begin + (beta_end - beta_begin) * (episode / beta_decay))\n","\n","\n","\"\"\"\n","    探索のためのパラメータε(episodeに依存)\n","\"\"\"\n","epsilon_begin = 1.0\n","epsilon_end = 0.05\n","epsilon_decay = n_episodes - 2000\n","# epsilon_beginから始めてepsilon_endまでepsilon_decayかけて線形に減らす\n","epsilon_func = lambda episode: max(epsilon_end, epsilon_begin - (epsilon_begin - epsilon_end) * (episode / epsilon_decay))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6MYas3233Q6f"},"source":["## 学習"]},{"cell_type":"code","metadata":{"id":"XnBB3exdUXr8"},"source":["# TensorBoardをColab内に起動\n","\n","tensorboard_path=os.path.join(each_dir,'logs3') # うまくtensorboardがでないときは、logs => logs○○と変えてあげて、後から手動でlogsに移動\n","%tensorboard --logdir=\"$tensorboard_path\" #--port 6060"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CULjQM_Pcfau"},"source":["# このセル以降のみを実行しても再現性があります。(すべてのセルを実行しなくてよいということ)\n","\n","# SEED値の固定\n","fix_seed(SEED)\n","first_player,env=make_game_env()\n","env.seed(SEED)\n","env.action_space.seed(SEED)\n","obs = env.reset()\n","\n","# replay_buffer & Network & Optimizerの宣言 (再現性のためにここで定義)\n","replay_buffer = PrioritizedReplayBuffer(buffer_size)\n","net = CNNQNetwork(env.observation_space.shape, n_action=env.action_space.n).to(device)\n","target_net = CNNQNetwork(env.observation_space.shape, n_action=env.action_space.n).to(device)\n","enemy_net = CNNQNetwork(env.observation_space.shape, n_action=env.action_space.n).to(device)\n","optimizer = optim.Adam(net.parameters(), lr=1e-4)  # オプティマイザはAdam\n","torchsummary.summary(net,obs.float().to(device).shape)\n","\n","# fine-tuning\n","if do_fine_tuning:\n","  net.load_state_dict(torch.load(load_weights_path))\n","  target_net.load_state_dict(torch.load(load_weights_path))\n","  enemy_net.load_state_dict(torch.load(load_weights_path))\n","  print(\"loaded weights\")\n","\n","# logとweightsを保存するフォルダの作成\n","JST = timezone(timedelta(hours=+9), 'JST')\n","now = datetime.now(JST)\n","log_path=os.path.join(tensorboard_path,now.strftime('%Y%m%d-%H%M%S') )\n","weights_path=os.path.join(log_path,\"weights\")\n","os.makedirs(weights_path)\n","\n","# tensorboard\n","writer = SummaryWriter(log_dir=log_path)\n","writer.add_graph(net,obs.float().to(device).unsqueeze(0))\n","\n","# 記録用にコピーを作成\n","shutil.copyfile(os.path.join(each_dir,\"ScoreFour.ipynb\"),os.path.join(log_path,\"for_record.ipynb\")) # each_dirの直下にScoreFour.ipynbという名前で置くこと\n","os.chmod(os.path.join(log_path,\"for_record.ipynb\"),SF_IMMUTABLE)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yJI1oieheawE"},"source":["# 再現性の確保\n","if (datetime.now(JST)-now).seconds > 10:\n","  raise Exception(\"前のセルと同時に実行しないと再現性が確保できません\")\n","\n","\n","info={\"turn\": first_player, \"winner\": 0}\n","\n","AGENT_TURN =  1\n","ENEMY_TURN = -1\n","\n","total_step = 0\n","total_reward = 0\n","enemy_update = 0\n","\n","win_num=0\n","lose_num=0\n","draw_num=0\n","\n","for episode in tqdm(range(n_episodes)):\n","  obs = env.reset()\n","  done = False\n","\n","  start_step = total_step\n","\n","  episode_reward=0\n","  episode_couldnt_locate_num=0\n","\n","  while not done:\n","    sum_reward = 0\n","    step_done=0\n","\n","    while step_done != 2:\n","      if (info[\"turn\"] == AGENT_TURN):\n","        before_action_obs=obs\n","        player_action = net.act(obs.float().to(device), epsilon_func(episode)) # ε-greedyで行動を選択\n","        next_obs, player_reward, done, info = env.step(player_action) # 環境中で実際に行動\n","        after_action_obs = next_obs\n","        total_step += 1\n","\n","        if info[\"is_couldnt_locate\"]==True:\n","          episode_reward += player_reward\n","          total_reward += player_reward\n","          replay_buffer.push([before_action_obs, player_action, player_reward, after_action_obs, done])# 置けなかったときのことを学習させる\n","          episode_couldnt_locate_num += 1\n","          # ネットワークを更新\n","          if len(replay_buffer) > initial_buffer_size:\n","            loss = update(batch_size, beta_func(episode))\n","            writer.add_scalar('Loss', loss, total_step)\n","        else:\n","          step_done+=1\n","          sum_reward += player_reward\n","\n","        if done :\n","          break\n","\n","      elif (info[\"turn\"] == ENEMY_TURN):\n","        if n_episodes/enemy_update_interval < enemy_update and random.random() < 0.7: # 常にgreedyだと一生置けないことがあるため確率性ももたせる\n","          enemy_action = enemy_net.act_greedy(obs.float().to(device)) # 相手はgreedy方策で行動選択\n","        else:\n","          enemy_action = env.action_space.sample() # ランダム\n","        next_obs, enemy_reward, done, info = env.step(enemy_action) # 環境中で実際に行動\n","        if info[\"is_couldnt_locate\"]==True:\n","          pass\n","        else:\n","          step_done+=1\n","        \n","        if (done):# 相手のcouldnt_locate_penaltyとcould_locate_rewardをsum_rewardに入れないように\n","          sum_reward -= enemy_reward # 相手が勝利して得た報酬を引く\n","          break\n","\n","      obs = next_obs\n","      \n","    # リプレイバッファに経験を蓄積\n","    replay_buffer.push([before_action_obs, player_action, sum_reward, after_action_obs, done])\n","\n","    episode_reward += sum_reward\n","    total_reward += sum_reward\n","\n","    \n","    # ネットワークを更新\n","    if len(replay_buffer) > initial_buffer_size:\n","      loss = update(batch_size, beta_func(episode))\n","      writer.add_scalar('Loss', loss, total_step)\n","    \n","    if done:\n","      if (info[\"winner\"] == AGENT_TURN):\n","        win_num+=1\n","      elif (info[\"winner\"] == ENEMY_TURN):\n","        lose_num+=1\n","      else:\n","        draw_num+=1\n","  \n","  episode_step = total_step-start_step\n","\n","  # 一定エピソードごとにコンソールに出力\n","  if ((episode+1) % 500 == 0):\n","    print('Episode: {},  TotalStep: {}, EpisodeStep: {},  EpisodeReward: {}'.format(episode + 1, total_step,episode_step, episode_reward))\n","  \n","  # validation\n","  if ((episode+1) % 2000 == 0):\n","    val_win_rate, val_draw_rate, val_lose_rate,val_couldnt_rate=validate_random(net,experiment_times=100,first_player=first_player)\n","    writer.add_scalar('Val-Win-Rate',val_win_rate, episode+1) \n","    writer.add_scalar('Val-Draw-Rate', val_draw_rate, episode+1) \n","    writer.add_scalar('Val-Lose-Rate', val_lose_rate, episode+1) \n","    writer.add_scalar('Val-Couldnt-Locate-Rate', val_couldnt_rate, episode+1) \n","    print(\"Win: {}%, Lose: {}%, Draw: {}%, couldnt: {}%\".format(val_win_rate, val_lose_rate,val_draw_rate, val_couldnt_rate))\n","\n","  # tensorboard用に記録\n","  writer.add_scalar('Total-Reward', total_reward, episode+1)\n","  writer.add_scalar('Episode-Reward', episode_reward, episode+1)\n","  writer.add_scalar('Episode-Step', episode_step, episode+1)\n","  writer.add_scalar('Win-Rate', win_num/(episode+1)*100, episode+1) \n","  writer.add_scalar('Draw-Rate', draw_num/(episode+1)*100, episode+1) \n","  writer.add_scalar('Lose-Rate', lose_num/(episode+1)*100, episode+1) \n","  writer.add_scalar('Episode-Couldnt-Locate-rate', episode_couldnt_locate_num/episode_step*100, episode+1)\n","  writer.add_scalar('Epsilon',epsilon_func(episode), episode+1)\n","\n","  # enemyネットワークを定期的に強くする\n","  if (episode + 1) % enemy_update_interval == 0:\n","      enemy_net.load_state_dict(target_net.state_dict())\n","      enemy_update += 1\n","\n","  # ターゲットネットワークを定期的に同期させる\n","  if (episode + 1) % target_update_interval == 0:\n","      target_net.load_state_dict(net.state_dict())\n","\n","  # networkの重みを定期的に保存\n","  if ((episode+1) % net_save_interval == 0):\n","    torch.save(net.state_dict(), weights_path+\"/weights_{}episodes.pth\".format(episode+1))\n","\n","torch.save(net.state_dict(), weights_path+\"/weights_final.pth\")\n","writer.close()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"soTLuxKgJVdb"},"source":["## 学習結果の確認"]},{"cell_type":"code","metadata":{"id":"3spwjjAvu9Wb"},"source":["first_player,env=make_game_env()\n","AGENT_TURN =  1\n","ENEMY_TURN = -1\n","\n","isCheckPreWeights = Truedddddddddd\n","time = \"20210319-164323\"\n","logs_folder=\"logs3\"\n","episode = 72000\n","episode = \"weights_{}episodes.pth\".format(episode)\n","load_weights_path = os.path.join(each_dir,logs_folder,time,\"weights\",episode)\n","if do_fine_tuning:\n","  net.load_state_dict(torch.load(load_weights_path))\n","  print(\"loaded weights\")\n","net = CNNQNetwork(env.observation_space.shape, n_action=env.action_space.n).to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UPuwven5XxDH"},"source":["experiment_times=300\n","\n","win_num=0\n","lose_num=0\n","draw_num=0\n","\n","log = []\n","\n","info={\"turn\": first_player, \"winner\": 0}\n","\n","total_step=0\n","total_couldnt_step=0\n","\n","for i in range(experiment_times):\n","  done=False\n","  log_child = []\n","  obs = env.reset()\n","\n","  # for animation\n","  episode_cube_history = []\n","  episode_cube_history.append(np.array(obs.squeeze(0)))\n","\n","  while not done:\n","    sum_reward = 0\n","    step_done=0\n","    isPreCouldntLocate=False\n","\n","    while step_done != 2:\n","      if (info[\"turn\"] == AGENT_TURN):\n","          if not isPreCouldntLocate: \n","            total_step+=1\n","            player_action = net.act(obs.float().to(device), 0) \n","            next_obs, player_reward, done, info = env.step(player_action) # 環境中で実際に行動\n","            sum_reward += player_reward\n","            if info[\"is_couldnt_locate\"]==True:\n","              total_couldnt_step+=1\n","              isPreCouldntLocate=True\n","              pass\n","            else:\n","              step_done+=1\n","          else:\n","            player_action = env.action_space.sample()\n","            next_obs, player_reward, done, info = env.step(player_action) # 環境中で実際に行動\n","            if info[\"is_couldnt_locate\"]==True:\n","              pass\n","            else:\n","              info[\"is_couldnt_locate\"]=True\n","              step_done+=1\n","          if done :\n","            episode_cube_history.append(np.array(next_obs.squeeze(0))) # for animation\n","            break\n","\n","      elif (info[\"turn\"] == ENEMY_TURN):\n","        enemy_action = env.action_space.sample() # ランダム\n","        next_obs, enemy_reward, done, info = env.step(enemy_action) # 環境中で実際に行動\n","        if info[\"is_couldnt_locate\"]==True:\n","          pass\n","        else:\n","          step_done+=1\n","        \n","        if (done):# 相手のcouldnt_locate_penaltyとcould_locate_rewardをsum_rewardに入れないように\n","          sum_reward -= enemy_reward # 相手が勝利して得た報酬を引く\n","          episode_cube_history.append(np.array(next_obs.squeeze(0))) # for animation\n","          break\n","\n","      obs = next_obs\n","      if info[\"is_couldnt_locate\"]==False:\n","        episode_cube_history.append(np.array(obs.squeeze(0))) # for animation\n","\n","    log_child.append([sum_reward,info])  \n","\n","  if(info[\"winner\"]==1):\n","    win_num+=1\n","  elif(info[\"winner\"]==-1):\n","    lose_num+=1\n","  else:\n","    draw_num+=1\n","\n","  log.append(log_child)\n","\n","  # 描画\n","  #env.render(mode=\"plot\",isClear=False)\n","  #env.animation(episode_cube_history)\n","\n","# logの出力\n","for i in range(len(log)):\n","  print()\n","  print(i)\n","  print(\"Reward, is_couldnt_locate, Winner\")\n","  for log_child in log[i]:\n","    print(\" {:6.2f}, {}, {}\".format(log_child[0],log_child[1][\"is_couldnt_locate\"],log_child[1][\"winner\"]))\n","\n","\n","print()\n","print(\"Result ({}times)\".format(experiment_times))\n","print(\"-------------------\")\n","win_rate=win_num/(experiment_times)*100\n","draw_rate=draw_num/(experiment_times)*100\n","lose_rate=lose_num/(experiment_times)*100\n","couldnt_rate=total_couldnt_step/(total_step)*100\n","print(\"win_rate:\",win_rate)\n","print(\"draw_rate:\",draw_rate) \n","print(\"lose_rate:\",lose_rate) \n","print(\"couldnt locate rate:\",couldnt_rate)\n","print(\"sum:\",win_rate+draw_rate+lose_rate)\n","\n","# result.mdの書き出し\n","#with open(os.path.join(log_path,\"result.md\"),\"w\") as f: \n","with open(os.path.join(each_dir,logs_folder,time,\"result.md\"),\"w\") as f:\n","  f.write(\"Result ({}times)\\n\".format(experiment_times))\n","  f.write(\"-------------------\\n\")\n","  f.write(\"win_rate: {}\\n\".format(win_rate))\n","  f.write(\"draw_rate: {}\\n\".format(draw_rate) )\n","  f.write(\"lose_rate: {}\\n\".format(lose_rate) )\n","  f.write(\"sum: {}\\n\".format(win_rate+draw_rate+lose_rate))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PBhE3aBBR-nk"},"source":["\n","# 記録用にコピーを作成\n","shutil.copyfile(os.path.join(each_dir,\"ScoreFour.ipynb\"),os.path.join(log_path,\"for_record_result.ipynb\")) # each_dirの直下にScoreFour.ipynbという名前で置くこと\n","os.chmod(os.path.join(log_path,\"for_record_result.ipynb\"),SF_IMMUTABLE)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f1Wlorc9TBc1"},"source":["## 研究メモ"]},{"cell_type":"markdown","metadata":{"id":"8XJsC4jyU4wg"},"source":["### エラー対処\n","- element 0 of tensors does not require grad and does not have a grad_fn\n","  - Reset Runtime"]}]}